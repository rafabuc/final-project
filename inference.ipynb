{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200b96b1",
   "metadata": {},
   "source": [
    "#Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b88ed30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 17:00:43 - InferencePipeline - INFO - Starting Multimodal Fake News Detection Inference\n",
      "[2025-12-09 17:00:43,205][InferencePipeline][INFO] - Starting Multimodal Fake News Detection Inference\n",
      "2025-12-09 17:00:43 - InferencePipeline - INFO - Configuration:\n",
      "model:\n",
      "  embedding_dim: 512\n",
      "  fusion_hidden_dim: 256\n",
      "  dropout_rate: 0.3\n",
      "  load_model: true\n",
      "  best_model_name: best_model_1.6.pth\n",
      "  freeze_vision: false\n",
      "  freeze_text: false\n",
      "  vision:\n",
      "    pretrained: true\n",
      "    backbone: efficientnet_b0\n",
      "  text:\n",
      "    pretrained_model: distilbert-base-uncased\n",
      "    hidden_size: 768\n",
      "data:\n",
      "  csv_file: /\n",
      "  root_dir: ../../data/\n",
      "  images_dir_train: ../../data/images/train/\n",
      "  images_dir_val: ../../data/images/val/\n",
      "  n_rows: 1000\n",
      "  text_column: text_content\n",
      "  image_column: image_path\n",
      "  label_column: label\n",
      "  train_split: 0.8\n",
      "  image_size: 224\n",
      "  max_length: 128\n",
      "  augmentation:\n",
      "    horizontal_flip: true\n",
      "    rotation_degrees: 15\n",
      "    color_jitter:\n",
      "      brightness: 0.2\n",
      "      contrast: 0.2\n",
      "      saturation: 0.2\n",
      "inference:\n",
      "  mode: single\n",
      "  checkpoint:\n",
      "    path: ./src/training/checkpoints/best_model_1.7.pth\n",
      "    device: null\n",
      "  single:\n",
      "    image_path: ./data/images/test/87pdmd.jpg\n",
      "    text: Saddam Hussein is executed (2006 Colorized)\n",
      "  batch:\n",
      "    csv_file: null\n",
      "    output_file: ./predictions.csv\n",
      "    root_dir: ./data/images/\n",
      "  text:\n",
      "    max_length: 128\n",
      "    tokenizer: distilbert-base-uncased\n",
      "  image:\n",
      "    size: 224\n",
      "    mode: val\n",
      "  output:\n",
      "    verbose: true\n",
      "    save_probabilities: true\n",
      "    save_confidence: true\n",
      "    threshold: 0.5\n",
      "  batch_processing:\n",
      "    batch_size: 32\n",
      "    num_workers: 4\n",
      "  freeze_vision: false\n",
      "  freeze_text: false\n",
      "  dropout_rate: 0.3\n",
      "\n",
      "[2025-12-09 17:00:43,209][InferencePipeline][INFO] - Configuration:\n",
      "model:\n",
      "  embedding_dim: 512\n",
      "  fusion_hidden_dim: 256\n",
      "  dropout_rate: 0.3\n",
      "  load_model: true\n",
      "  best_model_name: best_model_1.6.pth\n",
      "  freeze_vision: false\n",
      "  freeze_text: false\n",
      "  vision:\n",
      "    pretrained: true\n",
      "    backbone: efficientnet_b0\n",
      "  text:\n",
      "    pretrained_model: distilbert-base-uncased\n",
      "    hidden_size: 768\n",
      "data:\n",
      "  csv_file: /\n",
      "  root_dir: ../../data/\n",
      "  images_dir_train: ../../data/images/train/\n",
      "  images_dir_val: ../../data/images/val/\n",
      "  n_rows: 1000\n",
      "  text_column: text_content\n",
      "  image_column: image_path\n",
      "  label_column: label\n",
      "  train_split: 0.8\n",
      "  image_size: 224\n",
      "  max_length: 128\n",
      "  augmentation:\n",
      "    horizontal_flip: true\n",
      "    rotation_degrees: 15\n",
      "    color_jitter:\n",
      "      brightness: 0.2\n",
      "      contrast: 0.2\n",
      "      saturation: 0.2\n",
      "inference:\n",
      "  mode: single\n",
      "  checkpoint:\n",
      "    path: ./src/training/checkpoints/best_model_1.7.pth\n",
      "    device: null\n",
      "  single:\n",
      "    image_path: ./data/images/test/87pdmd.jpg\n",
      "    text: Saddam Hussein is executed (2006 Colorized)\n",
      "  batch:\n",
      "    csv_file: null\n",
      "    output_file: ./predictions.csv\n",
      "    root_dir: ./data/images/\n",
      "  text:\n",
      "    max_length: 128\n",
      "    tokenizer: distilbert-base-uncased\n",
      "  image:\n",
      "    size: 224\n",
      "    mode: val\n",
      "  output:\n",
      "    verbose: true\n",
      "    save_probabilities: true\n",
      "    save_confidence: true\n",
      "    threshold: 0.5\n",
      "  batch_processing:\n",
      "    batch_size: 32\n",
      "    num_workers: 4\n",
      "  freeze_vision: false\n",
      "  freeze_text: false\n",
      "  dropout_rate: 0.3\n",
      "\n",
      "2025-12-09 17:00:43 - InferencePipeline - INFO - Using device: cpu\n",
      "[2025-12-09 17:00:43,210][InferencePipeline][INFO] - Using device: cpu\n",
      "2025-12-09 17:00:43 - InferencePipeline - INFO - Loading model from ./src/training/checkpoints/best_model_1.7.pth...\n",
      "[2025-12-09 17:00:43,210][InferencePipeline][INFO] - Loading model from ./src/training/checkpoints/best_model_1.7.pth...\n",
      "2025-12-09 17:00:44 - InferencePipeline - INFO - Model loaded successfully! (Epoch: 4, Val Acc: 0.8420)\n",
      "[2025-12-09 17:00:44,528][InferencePipeline][INFO] - Model loaded successfully! (Epoch: 4, Val Acc: 0.8420)\n",
      "2025-12-09 17:00:44 - InferencePipeline - INFO - Loading tokenizer: distilbert-base-uncased\n",
      "[2025-12-09 17:00:44,528][InferencePipeline][INFO] - Loading tokenizer: distilbert-base-uncased\n",
      "2025-12-09 17:00:45 - InferencePipeline - INFO - \n",
      "=== Single Sample Prediction ===\n",
      "[2025-12-09 17:00:45,101][InferencePipeline][INFO] - \n",
      "=== Single Sample Prediction ===\n",
      "2025-12-09 17:00:45 - InferencePipeline - INFO - Image: ./data/images/test/87pdmd.jpg\n",
      "[2025-12-09 17:00:45,101][InferencePipeline][INFO] - Image: ./data/images/test/87pdmd.jpg\n",
      "2025-12-09 17:00:45 - InferencePipeline - INFO - Text: Saddam Hussein is executed (2006 Colorized)\n",
      "[2025-12-09 17:00:45,101][InferencePipeline][INFO] - Text: Saddam Hussein is executed (2006 Colorized)\n",
      "2025-12-09 17:00:45 - InferencePipeline - INFO - \n",
      "=== Results ===\n",
      "[2025-12-09 17:00:45,265][InferencePipeline][INFO] - \n",
      "=== Results ===\n",
      "2025-12-09 17:00:45 - InferencePipeline - INFO - Prediction: REAL\n",
      "[2025-12-09 17:00:45,265][InferencePipeline][INFO] - Prediction: REAL\n",
      "2025-12-09 17:00:45 - InferencePipeline - INFO - Probability (Fake): 0.1527\n",
      "[2025-12-09 17:00:45,265][InferencePipeline][INFO] - Probability (Fake): 0.1527\n",
      "2025-12-09 17:00:45 - InferencePipeline - INFO - Confidence: 0.8473\n",
      "[2025-12-09 17:00:45,265][InferencePipeline][INFO] - Confidence: 0.8473\n",
      "2025-12-09 17:00:45 - InferencePipeline - INFO - \n",
      "Inference completed successfully!\n",
      "[2025-12-09 17:00:45,266][InferencePipeline][INFO] - \n",
      "Inference completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\ml-zoomcamp\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\miniconda3\\envs\\ml-zoomcamp\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "!python inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b6867",
   "metadata": {},
   "source": [
    "#Run inference in batch mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f3061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 17:03:25 - InferencePipeline - INFO - Starting Multimodal Fake News Detection Inference\n",
      "[2025-12-09 17:03:25,058][InferencePipeline][INFO] - Starting Multimodal Fake News Detection Inference\n",
      "2025-12-09 17:03:25 - InferencePipeline - INFO - Configuration:\n",
      "model:\n",
      "  embedding_dim: 512\n",
      "  fusion_hidden_dim: 256\n",
      "  dropout_rate: 0.3\n",
      "  load_model: true\n",
      "  best_model_name: best_model_1.6.pth\n",
      "  freeze_vision: false\n",
      "  freeze_text: false\n",
      "  vision:\n",
      "    pretrained: true\n",
      "    backbone: efficientnet_b0\n",
      "  text:\n",
      "    pretrained_model: distilbert-base-uncased\n",
      "    hidden_size: 768\n",
      "data:\n",
      "  csv_file: /\n",
      "  root_dir: ../../data/\n",
      "  images_dir_train: ../../data/images/train/\n",
      "  images_dir_val: ../../data/images/val/\n",
      "  n_rows: 1000\n",
      "  text_column: text_content\n",
      "  image_column: image_path\n",
      "  label_column: label\n",
      "  train_split: 0.8\n",
      "  image_size: 224\n",
      "  max_length: 128\n",
      "  augmentation:\n",
      "    horizontal_flip: true\n",
      "    rotation_degrees: 15\n",
      "    color_jitter:\n",
      "      brightness: 0.2\n",
      "      contrast: 0.2\n",
      "      saturation: 0.2\n",
      "inference:\n",
      "  mode: single\n",
      "  checkpoint:\n",
      "    path: ./src/training/checkpoints/best_model_1.7.pth\n",
      "    device: null\n",
      "  single:\n",
      "    image_path: ./data/images/test/c7jxj5.jpg\n",
      "    text: This frog sitting on a light\n",
      "  batch:\n",
      "    csv_file: null\n",
      "    output_file: ./predictions.csv\n",
      "    root_dir: ./data/images/\n",
      "  text:\n",
      "    max_length: 128\n",
      "    tokenizer: distilbert-base-uncased\n",
      "  image:\n",
      "    size: 224\n",
      "    mode: val\n",
      "  output:\n",
      "    verbose: true\n",
      "    save_probabilities: true\n",
      "    save_confidence: true\n",
      "    threshold: 0.5\n",
      "  batch_processing:\n",
      "    batch_size: 32\n",
      "    num_workers: 4\n",
      "  freeze_vision: false\n",
      "  freeze_text: false\n",
      "  dropout_rate: 0.3\n",
      "\n",
      "[2025-12-09 17:03:25,062][InferencePipeline][INFO] - Configuration:\n",
      "model:\n",
      "  embedding_dim: 512\n",
      "  fusion_hidden_dim: 256\n",
      "  dropout_rate: 0.3\n",
      "  load_model: true\n",
      "  best_model_name: best_model_1.6.pth\n",
      "  freeze_vision: false\n",
      "  freeze_text: false\n",
      "  vision:\n",
      "    pretrained: true\n",
      "    backbone: efficientnet_b0\n",
      "  text:\n",
      "    pretrained_model: distilbert-base-uncased\n",
      "    hidden_size: 768\n",
      "data:\n",
      "  csv_file: /\n",
      "  root_dir: ../../data/\n",
      "  images_dir_train: ../../data/images/train/\n",
      "  images_dir_val: ../../data/images/val/\n",
      "  n_rows: 1000\n",
      "  text_column: text_content\n",
      "  image_column: image_path\n",
      "  label_column: label\n",
      "  train_split: 0.8\n",
      "  image_size: 224\n",
      "  max_length: 128\n",
      "  augmentation:\n",
      "    horizontal_flip: true\n",
      "    rotation_degrees: 15\n",
      "    color_jitter:\n",
      "      brightness: 0.2\n",
      "      contrast: 0.2\n",
      "      saturation: 0.2\n",
      "inference:\n",
      "  mode: single\n",
      "  checkpoint:\n",
      "    path: ./src/training/checkpoints/best_model_1.7.pth\n",
      "    device: null\n",
      "  single:\n",
      "    image_path: ./data/images/test/c7jxj5.jpg\n",
      "    text: This frog sitting on a light\n",
      "  batch:\n",
      "    csv_file: null\n",
      "    output_file: ./predictions.csv\n",
      "    root_dir: ./data/images/\n",
      "  text:\n",
      "    max_length: 128\n",
      "    tokenizer: distilbert-base-uncased\n",
      "  image:\n",
      "    size: 224\n",
      "    mode: val\n",
      "  output:\n",
      "    verbose: true\n",
      "    save_probabilities: true\n",
      "    save_confidence: true\n",
      "    threshold: 0.5\n",
      "  batch_processing:\n",
      "    batch_size: 32\n",
      "    num_workers: 4\n",
      "  freeze_vision: false\n",
      "  freeze_text: false\n",
      "  dropout_rate: 0.3\n",
      "\n",
      "2025-12-09 17:03:25 - InferencePipeline - INFO - Using device: cpu\n",
      "[2025-12-09 17:03:25,063][InferencePipeline][INFO] - Using device: cpu\n",
      "2025-12-09 17:03:25 - InferencePipeline - INFO - Loading model from ./src/training/checkpoints/best_model_1.7.pth...\n",
      "[2025-12-09 17:03:25,063][InferencePipeline][INFO] - Loading model from ./src/training/checkpoints/best_model_1.7.pth...\n",
      "2025-12-09 17:03:26 - InferencePipeline - INFO - Model loaded successfully! (Epoch: 4, Val Acc: 0.8420)\n",
      "[2025-12-09 17:03:26,394][InferencePipeline][INFO] - Model loaded successfully! (Epoch: 4, Val Acc: 0.8420)\n",
      "2025-12-09 17:03:26 - InferencePipeline - INFO - Loading tokenizer: distilbert-base-uncased\n",
      "[2025-12-09 17:03:26,394][InferencePipeline][INFO] - Loading tokenizer: distilbert-base-uncased\n",
      "2025-12-09 17:03:26 - InferencePipeline - INFO - \n",
      "=== Single Sample Prediction ===\n",
      "[2025-12-09 17:03:26,960][InferencePipeline][INFO] - \n",
      "=== Single Sample Prediction ===\n",
      "2025-12-09 17:03:26 - InferencePipeline - INFO - Image: ./data/images/test/c7jxj5.jpg\n",
      "[2025-12-09 17:03:26,960][InferencePipeline][INFO] - Image: ./data/images/test/c7jxj5.jpg\n",
      "2025-12-09 17:03:26 - InferencePipeline - INFO - Text: This frog sitting on a light\n",
      "[2025-12-09 17:03:26,960][InferencePipeline][INFO] - Text: This frog sitting on a light\n",
      "2025-12-09 17:03:27 - InferencePipeline - INFO - \n",
      "=== Results ===\n",
      "[2025-12-09 17:03:27,129][InferencePipeline][INFO] - \n",
      "=== Results ===\n",
      "2025-12-09 17:03:27 - InferencePipeline - INFO - Prediction: FAKE\n",
      "[2025-12-09 17:03:27,129][InferencePipeline][INFO] - Prediction: FAKE\n",
      "2025-12-09 17:03:27 - InferencePipeline - INFO - Probability (Fake): 0.6505\n",
      "[2025-12-09 17:03:27,129][InferencePipeline][INFO] - Probability (Fake): 0.6505\n",
      "2025-12-09 17:03:27 - InferencePipeline - INFO - Confidence: 0.6505\n",
      "[2025-12-09 17:03:27,129][InferencePipeline][INFO] - Confidence: 0.6505\n",
      "2025-12-09 17:03:27 - InferencePipeline - INFO - \n",
      "Inference completed successfully!\n",
      "[2025-12-09 17:03:27,130][InferencePipeline][INFO] - \n",
      "Inference completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\ml-zoomcamp\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\miniconda3\\envs\\ml-zoomcamp\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#mode: running in single mode\n",
    "!python inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e532337b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 17:12:13 - InferencePipeline - INFO - Starting Multimodal Fake News Detection Inference\n",
      "[2025-12-09 17:12:13,265][InferencePipeline][INFO] - Starting Multimodal Fake News Detection Inference\n",
      "2025-12-09 17:12:13 - InferencePipeline - INFO - Configuration:\n",
      "model:\n",
      "  embedding_dim: 512\n",
      "  fusion_hidden_dim: 256\n",
      "  dropout_rate: 0.3\n",
      "  load_model: true\n",
      "  best_model_name: best_model_1.6.pth\n",
      "  freeze_vision: false\n",
      "  freeze_text: false\n",
      "  vision:\n",
      "    pretrained: true\n",
      "    backbone: efficientnet_b0\n",
      "  text:\n",
      "    pretrained_model: distilbert-base-uncased\n",
      "    hidden_size: 768\n",
      "data:\n",
      "  csv_file: /\n",
      "  root_dir: ../../data/\n",
      "  images_dir_train: ../../data/images/train/\n",
      "  images_dir_val: ../../data/images/val/\n",
      "  n_rows: 1000\n",
      "  text_column: text_content\n",
      "  image_column: image_path\n",
      "  label_column: label\n",
      "  train_split: 0.8\n",
      "  image_size: 224\n",
      "  max_length: 128\n",
      "  augmentation:\n",
      "    horizontal_flip: true\n",
      "    rotation_degrees: 15\n",
      "    color_jitter:\n",
      "      brightness: 0.2\n",
      "      contrast: 0.2\n",
      "      saturation: 0.2\n",
      "inference:\n",
      "  mode: batch\n",
      "  checkpoint:\n",
      "    path: ./src/training/checkpoints/best_model_1.7.pth\n",
      "    device: null\n",
      "  single:\n",
      "    image_path: ./data/images/test/c7jxj5.jpg\n",
      "    text: This frog sitting on a light\n",
      "  batch:\n",
      "    csv_file: ./data/test.csv\n",
      "    output_file: ./data/test-predictions.csv\n",
      "    root_dir: ./data/images/test/\n",
      "    n_rows: 200\n",
      "  text:\n",
      "    max_length: 128\n",
      "    tokenizer: distilbert-base-uncased\n",
      "  image:\n",
      "    size: 224\n",
      "    mode: val\n",
      "  output:\n",
      "    verbose: true\n",
      "    save_probabilities: true\n",
      "    save_confidence: true\n",
      "    threshold: 0.5\n",
      "  batch_processing:\n",
      "    batch_size: 32\n",
      "    num_workers: 4\n",
      "  freeze_vision: false\n",
      "  freeze_text: false\n",
      "  dropout_rate: 0.3\n",
      "\n",
      "[2025-12-09 17:12:13,269][InferencePipeline][INFO] - Configuration:\n",
      "model:\n",
      "  embedding_dim: 512\n",
      "  fusion_hidden_dim: 256\n",
      "  dropout_rate: 0.3\n",
      "  load_model: true\n",
      "  best_model_name: best_model_1.6.pth\n",
      "  freeze_vision: false\n",
      "  freeze_text: false\n",
      "  vision:\n",
      "    pretrained: true\n",
      "    backbone: efficientnet_b0\n",
      "  text:\n",
      "    pretrained_model: distilbert-base-uncased\n",
      "    hidden_size: 768\n",
      "data:\n",
      "  csv_file: /\n",
      "  root_dir: ../../data/\n",
      "  images_dir_train: ../../data/images/train/\n",
      "  images_dir_val: ../../data/images/val/\n",
      "  n_rows: 1000\n",
      "  text_column: text_content\n",
      "  image_column: image_path\n",
      "  label_column: label\n",
      "  train_split: 0.8\n",
      "  image_size: 224\n",
      "  max_length: 128\n",
      "  augmentation:\n",
      "    horizontal_flip: true\n",
      "    rotation_degrees: 15\n",
      "    color_jitter:\n",
      "      brightness: 0.2\n",
      "      contrast: 0.2\n",
      "      saturation: 0.2\n",
      "inference:\n",
      "  mode: batch\n",
      "  checkpoint:\n",
      "    path: ./src/training/checkpoints/best_model_1.7.pth\n",
      "    device: null\n",
      "  single:\n",
      "    image_path: ./data/images/test/c7jxj5.jpg\n",
      "    text: This frog sitting on a light\n",
      "  batch:\n",
      "    csv_file: ./data/test.csv\n",
      "    output_file: ./data/test-predictions.csv\n",
      "    root_dir: ./data/images/test/\n",
      "    n_rows: 200\n",
      "  text:\n",
      "    max_length: 128\n",
      "    tokenizer: distilbert-base-uncased\n",
      "  image:\n",
      "    size: 224\n",
      "    mode: val\n",
      "  output:\n",
      "    verbose: true\n",
      "    save_probabilities: true\n",
      "    save_confidence: true\n",
      "    threshold: 0.5\n",
      "  batch_processing:\n",
      "    batch_size: 32\n",
      "    num_workers: 4\n",
      "  freeze_vision: false\n",
      "  freeze_text: false\n",
      "  dropout_rate: 0.3\n",
      "\n",
      "2025-12-09 17:12:13 - InferencePipeline - INFO - Using device: cpu\n",
      "[2025-12-09 17:12:13,270][InferencePipeline][INFO] - Using device: cpu\n",
      "2025-12-09 17:12:13 - InferencePipeline - INFO - Loading model from ./src/training/checkpoints/best_model_1.7.pth...\n",
      "[2025-12-09 17:12:13,270][InferencePipeline][INFO] - Loading model from ./src/training/checkpoints/best_model_1.7.pth...\n",
      "2025-12-09 17:12:15 - InferencePipeline - INFO - Model loaded successfully! (Epoch: 4, Val Acc: 0.8420)\n",
      "[2025-12-09 17:12:15,819][InferencePipeline][INFO] - Model loaded successfully! (Epoch: 4, Val Acc: 0.8420)\n",
      "2025-12-09 17:12:15 - InferencePipeline - INFO - Loading tokenizer: distilbert-base-uncased\n",
      "[2025-12-09 17:12:15,820][InferencePipeline][INFO] - Loading tokenizer: distilbert-base-uncased\n",
      "2025-12-09 17:12:16 - InferencePipeline - INFO - \n",
      "=== Batch Prediction ===\n",
      "[2025-12-09 17:12:16,742][InferencePipeline][INFO] - \n",
      "=== Batch Prediction ===\n",
      "2025-12-09 17:12:16 - InferencePipeline - INFO - Loading data from ./data/test.csv...\n",
      "[2025-12-09 17:12:16,742][InferencePipeline][INFO] - Loading data from ./data/test.csv...\n",
      "2025-12-09 17:12:16 - InferencePipeline - INFO - Processing 200 samples...\n",
      "[2025-12-09 17:12:16,764][InferencePipeline][INFO] - Processing 200 samples...\n",
      "2025-12-09 17:13:02 - InferencePipeline - INFO - Predictions saved to ./data/test-predictions.csv\n",
      "[2025-12-09 17:13:02,737][InferencePipeline][INFO] - Predictions saved to ./data/test-predictions.csv\n",
      "2025-12-09 17:13:02 - InferencePipeline - INFO - \n",
      "=== Prediction Summary ===\n",
      "[2025-12-09 17:13:02,737][InferencePipeline][INFO] - \n",
      "=== Prediction Summary ===\n",
      "2025-12-09 17:13:02 - InferencePipeline - INFO - Total samples: 200\n",
      "[2025-12-09 17:13:02,737][InferencePipeline][INFO] - Total samples: 200\n",
      "2025-12-09 17:13:02 - InferencePipeline - INFO - Predicted FAKE: 124\n",
      "[2025-12-09 17:13:02,742][InferencePipeline][INFO] - Predicted FAKE: 124\n",
      "2025-12-09 17:13:02 - InferencePipeline - INFO - Predicted REAL: 76\n",
      "[2025-12-09 17:13:02,743][InferencePipeline][INFO] - Predicted REAL: 76\n",
      "2025-12-09 17:13:02 - InferencePipeline - INFO - Average confidence: 0.8407\n",
      "[2025-12-09 17:13:02,745][InferencePipeline][INFO] - Average confidence: 0.8407\n",
      "2025-12-09 17:13:02 - InferencePipeline - INFO - \n",
      "=== Sample Predictions ===\n",
      "[2025-12-09 17:13:02,745][InferencePipeline][INFO] - \n",
      "=== Sample Predictions ===\n",
      "  prediction  probability  confidence\n",
      "0       REAL     0.290626    0.709374\n",
      "1       FAKE     0.975734    0.975734\n",
      "2       FAKE     0.961336    0.961336\n",
      "3       REAL     0.452763    0.547237\n",
      "4       REAL     0.152737    0.847263\n",
      "5       FAKE     0.826974    0.826974\n",
      "6       FAKE     0.832330    0.832330\n",
      "7       FAKE     0.962145    0.962145\n",
      "8       FAKE     0.955734    0.955734\n",
      "9       REAL     0.230728    0.769272\n",
      "2025-12-09 17:13:02 - InferencePipeline - INFO - \n",
      "Inference completed successfully!\n",
      "[2025-12-09 17:13:02,762][InferencePipeline][INFO] - \n",
      "Inference completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\ml-zoomcamp\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\miniconda3\\envs\\ml-zoomcamp\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "\n",
      "Predicting:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Predicting:   0%|          | 1/200 [00:00<00:35,  5.54it/s]\n",
      "Predicting:   1%|          | 2/200 [00:00<00:34,  5.66it/s]\n",
      "Predicting:   2%|▏         | 3/200 [00:00<00:33,  5.90it/s]\n",
      "Predicting:   2%|▏         | 4/200 [00:00<00:32,  6.01it/s]\n",
      "Predicting:   2%|▎         | 5/200 [00:00<00:30,  6.38it/s]\n",
      "Predicting:   3%|▎         | 6/200 [00:00<00:29,  6.56it/s]\n",
      "Predicting:   4%|▎         | 7/200 [00:01<00:29,  6.65it/s]\n",
      "Predicting:   4%|▍         | 8/200 [00:01<00:28,  6.72it/s]\n",
      "Predicting:   4%|▍         | 9/200 [00:01<00:29,  6.49it/s]\n",
      "Predicting:   5%|▌         | 10/200 [00:01<00:30,  6.16it/s]\n",
      "Predicting:   6%|▌         | 11/200 [00:02<00:55,  3.43it/s]\n",
      "Predicting:   6%|▌         | 12/200 [00:02<00:48,  3.88it/s]\n",
      "Predicting:   6%|▋         | 13/200 [00:02<00:42,  4.36it/s]\n",
      "Predicting:   7%|▋         | 14/200 [00:02<00:39,  4.75it/s]\n",
      "Predicting:   8%|▊         | 15/200 [00:02<00:35,  5.26it/s]\n",
      "Predicting:   8%|▊         | 16/200 [00:03<00:34,  5.37it/s]\n",
      "Predicting:   8%|▊         | 17/200 [00:03<00:33,  5.42it/s]\n",
      "Predicting:   9%|▉         | 18/200 [00:03<00:31,  5.72it/s]\n",
      "Predicting:  10%|▉         | 19/200 [00:03<00:31,  5.73it/s]\n",
      "Predicting:  10%|█         | 20/200 [00:03<00:30,  5.90it/s]\n",
      "Predicting:  10%|█         | 21/200 [00:03<00:30,  5.78it/s]\n",
      "Predicting:  11%|█         | 22/200 [00:04<00:30,  5.91it/s]\n",
      "Predicting:  12%|█▏        | 23/200 [00:04<00:29,  5.97it/s]\n",
      "Predicting:  12%|█▏        | 24/200 [00:04<00:28,  6.13it/s]\n",
      "Predicting:  12%|█▎        | 25/200 [00:04<00:28,  6.17it/s]\n",
      "Predicting:  13%|█▎        | 26/200 [00:04<00:37,  4.70it/s]\n",
      "Predicting:  14%|█▎        | 27/200 [00:05<00:43,  3.99it/s]\n",
      "Predicting:  14%|█▍        | 28/200 [00:05<00:37,  4.53it/s]\n",
      "Predicting:  14%|█▍        | 29/200 [00:05<00:34,  4.95it/s]\n",
      "Predicting:  15%|█▌        | 30/200 [00:05<00:32,  5.30it/s]\n",
      "Predicting:  16%|█▌        | 31/200 [00:05<00:30,  5.50it/s]\n",
      "Predicting:  16%|█▌        | 32/200 [00:05<00:29,  5.66it/s]\n",
      "Predicting:  16%|█▋        | 33/200 [00:06<00:31,  5.25it/s]\n",
      "Predicting:  17%|█▋        | 34/200 [00:06<00:41,  3.98it/s]\n",
      "Predicting:  18%|█▊        | 35/200 [00:06<00:48,  3.39it/s]\n",
      "Predicting:  18%|█▊        | 36/200 [00:07<00:44,  3.71it/s]\n",
      "Predicting:  18%|█▊        | 37/200 [00:07<00:42,  3.83it/s]\n",
      "Predicting:  19%|█▉        | 38/200 [00:07<00:40,  4.02it/s]\n",
      "Predicting:  20%|█▉        | 39/200 [00:07<00:37,  4.27it/s]\n",
      "Predicting:  20%|██        | 40/200 [00:08<00:33,  4.73it/s]\n",
      "Predicting:  20%|██        | 41/200 [00:08<00:31,  5.00it/s]\n",
      "Predicting:  21%|██        | 42/200 [00:08<00:29,  5.35it/s]\n",
      "Predicting:  22%|██▏       | 43/200 [00:08<00:30,  5.15it/s]\n",
      "Predicting:  22%|██▏       | 44/200 [00:08<00:32,  4.87it/s]\n",
      "Predicting:  22%|██▎       | 45/200 [00:09<00:44,  3.49it/s]\n",
      "Predicting:  23%|██▎       | 46/200 [00:09<00:39,  3.93it/s]\n",
      "Predicting:  24%|██▎       | 47/200 [00:09<00:34,  4.38it/s]\n",
      "Predicting:  24%|██▍       | 48/200 [00:09<00:31,  4.86it/s]\n",
      "Predicting:  24%|██▍       | 49/200 [00:10<00:33,  4.44it/s]\n",
      "Predicting:  25%|██▌       | 50/200 [00:10<00:31,  4.74it/s]\n",
      "Predicting:  26%|██▌       | 51/200 [00:10<00:29,  4.99it/s]\n",
      "Predicting:  26%|██▌       | 52/200 [00:10<00:30,  4.89it/s]\n",
      "Predicting:  26%|██▋       | 53/200 [00:10<00:28,  5.15it/s]\n",
      "Predicting:  27%|██▋       | 54/200 [00:10<00:26,  5.44it/s]\n",
      "Predicting:  28%|██▊       | 55/200 [00:11<00:27,  5.23it/s]\n",
      "Predicting:  28%|██▊       | 56/200 [00:11<00:30,  4.66it/s]\n",
      "Predicting:  28%|██▊       | 57/200 [00:11<00:29,  4.92it/s]\n",
      "Predicting:  29%|██▉       | 58/200 [00:11<00:27,  5.24it/s]\n",
      "Predicting:  30%|██▉       | 59/200 [00:11<00:25,  5.49it/s]\n",
      "Predicting:  30%|███       | 60/200 [00:12<00:24,  5.64it/s]\n",
      "Predicting:  30%|███       | 61/200 [00:12<00:26,  5.32it/s]\n",
      "Predicting:  31%|███       | 62/200 [00:12<00:39,  3.48it/s]\n",
      "Predicting:  32%|███▏      | 63/200 [00:12<00:33,  4.04it/s]\n",
      "Predicting:  32%|███▏      | 64/200 [00:13<00:29,  4.54it/s]\n",
      "Predicting:  32%|███▎      | 65/200 [00:13<00:32,  4.19it/s]\n",
      "Predicting:  33%|███▎      | 66/200 [00:13<00:44,  3.04it/s]\n",
      "Predicting:  34%|███▎      | 67/200 [00:14<00:36,  3.63it/s]\n",
      "Predicting:  34%|███▍      | 68/200 [00:14<00:34,  3.80it/s]\n",
      "Predicting:  34%|███▍      | 69/200 [00:14<00:31,  4.15it/s]\n",
      "Predicting:  35%|███▌      | 70/200 [00:14<00:28,  4.52it/s]\n",
      "Predicting:  36%|███▌      | 71/200 [00:14<00:28,  4.53it/s]\n",
      "Predicting:  36%|███▌      | 72/200 [00:15<00:28,  4.55it/s]\n",
      "Predicting:  36%|███▋      | 73/200 [00:15<00:26,  4.83it/s]\n",
      "Predicting:  37%|███▋      | 74/200 [00:15<00:26,  4.74it/s]\n",
      "Predicting:  38%|███▊      | 75/200 [00:15<00:26,  4.71it/s]\n",
      "Predicting:  38%|███▊      | 76/200 [00:15<00:26,  4.71it/s]\n",
      "Predicting:  38%|███▊      | 77/200 [00:16<00:34,  3.60it/s]\n",
      "Predicting:  39%|███▉      | 78/200 [00:16<00:35,  3.47it/s]\n",
      "Predicting:  40%|███▉      | 79/200 [00:16<00:31,  3.89it/s]\n",
      "Predicting:  40%|████      | 80/200 [00:17<00:29,  4.03it/s]\n",
      "Predicting:  40%|████      | 81/200 [00:17<00:28,  4.15it/s]\n",
      "Predicting:  41%|████      | 82/200 [00:17<00:26,  4.48it/s]\n",
      "Predicting:  42%|████▏     | 83/200 [00:17<00:24,  4.72it/s]\n",
      "Predicting:  42%|████▏     | 84/200 [00:17<00:25,  4.59it/s]\n",
      "Predicting:  42%|████▎     | 85/200 [00:18<00:41,  2.77it/s]\n",
      "Predicting:  43%|████▎     | 86/200 [00:19<00:44,  2.59it/s]\n",
      "Predicting:  44%|████▎     | 87/200 [00:19<00:37,  3.00it/s]\n",
      "Predicting:  44%|████▍     | 88/200 [00:19<00:32,  3.44it/s]\n",
      "Predicting:  44%|████▍     | 89/200 [00:19<00:29,  3.74it/s]\n",
      "Predicting:  45%|████▌     | 90/200 [00:19<00:29,  3.73it/s]\n",
      "Predicting:  46%|████▌     | 91/200 [00:20<00:28,  3.80it/s]\n",
      "Predicting:  46%|████▌     | 92/200 [00:20<00:26,  4.06it/s]\n",
      "Predicting:  46%|████▋     | 93/200 [00:20<00:26,  4.06it/s]\n",
      "Predicting:  47%|████▋     | 94/200 [00:21<00:34,  3.04it/s]\n",
      "Predicting:  48%|████▊     | 95/200 [00:21<00:31,  3.34it/s]\n",
      "Predicting:  48%|████▊     | 96/200 [00:21<00:28,  3.62it/s]\n",
      "Predicting:  48%|████▊     | 97/200 [00:21<00:26,  3.90it/s]\n",
      "Predicting:  49%|████▉     | 98/200 [00:22<00:26,  3.91it/s]\n",
      "Predicting:  50%|████▉     | 99/200 [00:22<00:23,  4.24it/s]\n",
      "Predicting:  50%|█████     | 100/200 [00:22<00:23,  4.27it/s]\n",
      "Predicting:  50%|█████     | 101/200 [00:22<00:29,  3.30it/s]\n",
      "Predicting:  51%|█████     | 102/200 [00:23<00:26,  3.64it/s]\n",
      "Predicting:  52%|█████▏    | 103/200 [00:23<00:24,  4.02it/s]\n",
      "Predicting:  52%|█████▏    | 104/200 [00:23<00:24,  3.88it/s]\n",
      "Predicting:  52%|█████▎    | 105/200 [00:23<00:23,  3.98it/s]\n",
      "Predicting:  53%|█████▎    | 106/200 [00:24<00:21,  4.29it/s]\n",
      "Predicting:  54%|█████▎    | 107/200 [00:24<00:20,  4.53it/s]\n",
      "Predicting:  54%|█████▍    | 108/200 [00:24<00:19,  4.69it/s]\n",
      "Predicting:  55%|█████▍    | 109/200 [00:24<00:18,  4.95it/s]\n",
      "Predicting:  55%|█████▌    | 110/200 [00:24<00:19,  4.61it/s]\n",
      "Predicting:  56%|█████▌    | 111/200 [00:25<00:29,  3.06it/s]\n",
      "Predicting:  56%|█████▌    | 112/200 [00:25<00:26,  3.37it/s]\n",
      "Predicting:  56%|█████▋    | 113/200 [00:25<00:23,  3.78it/s]\n",
      "Predicting:  57%|█████▋    | 114/200 [00:26<00:20,  4.15it/s]\n",
      "Predicting:  57%|█████▊    | 115/200 [00:26<00:19,  4.46it/s]\n",
      "Predicting:  58%|█████▊    | 116/200 [00:26<00:18,  4.61it/s]\n",
      "Predicting:  58%|█████▊    | 117/200 [00:26<00:17,  4.61it/s]\n",
      "Predicting:  59%|█████▉    | 118/200 [00:26<00:16,  5.04it/s]\n",
      "Predicting:  60%|█████▉    | 119/200 [00:26<00:15,  5.39it/s]\n",
      "Predicting:  60%|██████    | 120/200 [00:27<00:13,  5.81it/s]\n",
      "Predicting:  60%|██████    | 121/200 [00:27<00:12,  6.12it/s]\n",
      "Predicting:  61%|██████    | 122/200 [00:27<00:12,  6.36it/s]\n",
      "Predicting:  62%|██████▏   | 123/200 [00:27<00:11,  6.56it/s]\n",
      "Predicting:  62%|██████▏   | 124/200 [00:27<00:11,  6.69it/s]\n",
      "Predicting:  62%|██████▎   | 125/200 [00:27<00:11,  6.77it/s]\n",
      "Predicting:  63%|██████▎   | 126/200 [00:28<00:11,  6.55it/s]\n",
      "Predicting:  64%|██████▎   | 127/200 [00:28<00:11,  6.52it/s]\n",
      "Predicting:  64%|██████▍   | 128/200 [00:28<00:15,  4.68it/s]\n",
      "Predicting:  64%|██████▍   | 129/200 [00:28<00:13,  5.12it/s]\n",
      "Predicting:  65%|██████▌   | 130/200 [00:28<00:12,  5.53it/s]\n",
      "Predicting:  66%|██████▌   | 131/200 [00:28<00:11,  5.90it/s]\n",
      "Predicting:  66%|██████▌   | 132/200 [00:29<00:10,  6.25it/s]\n",
      "Predicting:  66%|██████▋   | 133/200 [00:29<00:10,  6.47it/s]\n",
      "Predicting:  67%|██████▋   | 134/200 [00:29<00:09,  6.68it/s]\n",
      "Predicting:  68%|██████▊   | 135/200 [00:29<00:09,  6.94it/s]\n",
      "Predicting:  68%|██████▊   | 136/200 [00:29<00:09,  6.75it/s]\n",
      "Predicting:  68%|██████▊   | 137/200 [00:29<00:09,  6.58it/s]\n",
      "Predicting:  69%|██████▉   | 138/200 [00:29<00:09,  6.70it/s]\n",
      "Predicting:  70%|██████▉   | 139/200 [00:30<00:08,  6.82it/s]\n",
      "Predicting:  70%|███████   | 140/200 [00:30<00:08,  6.86it/s]\n",
      "Predicting:  70%|███████   | 141/200 [00:30<00:08,  6.91it/s]\n",
      "Predicting:  71%|███████   | 142/200 [00:30<00:08,  7.00it/s]\n",
      "Predicting:  72%|███████▏  | 143/200 [00:30<00:08,  7.04it/s]\n",
      "Predicting:  72%|███████▏  | 144/200 [00:30<00:07,  7.12it/s]\n",
      "Predicting:  72%|███████▎  | 145/200 [00:30<00:07,  7.04it/s]\n",
      "Predicting:  73%|███████▎  | 146/200 [00:31<00:07,  7.07it/s]\n",
      "Predicting:  74%|███████▎  | 147/200 [00:31<00:07,  7.07it/s]\n",
      "Predicting:  74%|███████▍  | 148/200 [00:31<00:09,  5.71it/s]\n",
      "Predicting:  74%|███████▍  | 149/200 [00:31<00:12,  4.09it/s]\n",
      "Predicting:  75%|███████▌  | 150/200 [00:32<00:14,  3.36it/s]\n",
      "Predicting:  76%|███████▌  | 151/200 [00:32<00:12,  3.93it/s]\n",
      "Predicting:  76%|███████▌  | 152/200 [00:32<00:10,  4.58it/s]\n",
      "Predicting:  76%|███████▋  | 153/200 [00:32<00:09,  5.17it/s]\n",
      "Predicting:  77%|███████▋  | 154/200 [00:32<00:08,  5.74it/s]\n",
      "Predicting:  78%|███████▊  | 155/200 [00:33<00:07,  5.90it/s]\n",
      "Predicting:  78%|███████▊  | 156/200 [00:33<00:07,  5.81it/s]\n",
      "Predicting:  78%|███████▊  | 157/200 [00:33<00:08,  4.79it/s]\n",
      "Predicting:  79%|███████▉  | 158/200 [00:33<00:09,  4.45it/s]\n",
      "Predicting:  80%|███████▉  | 159/200 [00:34<00:11,  3.59it/s]\n",
      "Predicting:  80%|████████  | 160/200 [00:34<00:13,  2.98it/s]\n",
      "Predicting:  80%|████████  | 161/200 [00:34<00:11,  3.41it/s]\n",
      "Predicting:  81%|████████  | 162/200 [00:34<00:09,  3.93it/s]\n",
      "Predicting:  82%|████████▏ | 163/200 [00:35<00:08,  4.36it/s]\n",
      "Predicting:  82%|████████▏ | 164/200 [00:35<00:08,  4.46it/s]\n",
      "Predicting:  82%|████████▎ | 165/200 [00:35<00:07,  4.98it/s]\n",
      "Predicting:  83%|████████▎ | 166/200 [00:35<00:06,  4.94it/s]\n",
      "Predicting:  84%|████████▎ | 167/200 [00:36<00:07,  4.20it/s]\n",
      "Predicting:  84%|████████▍ | 168/200 [00:36<00:08,  3.63it/s]\n",
      "Predicting:  84%|████████▍ | 169/200 [00:36<00:10,  3.08it/s]\n",
      "Predicting:  85%|████████▌ | 170/200 [00:37<00:09,  3.15it/s]\n",
      "Predicting:  86%|████████▌ | 171/200 [00:37<00:08,  3.41it/s]\n",
      "Predicting:  86%|████████▌ | 172/200 [00:37<00:07,  3.70it/s]\n",
      "Predicting:  86%|████████▋ | 173/200 [00:37<00:06,  3.92it/s]\n",
      "Predicting:  87%|████████▋ | 174/200 [00:38<00:06,  4.19it/s]\n",
      "Predicting:  88%|████████▊ | 175/200 [00:38<00:05,  4.41it/s]\n",
      "Predicting:  88%|████████▊ | 176/200 [00:38<00:05,  4.55it/s]\n",
      "Predicting:  88%|████████▊ | 177/200 [00:38<00:04,  4.76it/s]\n",
      "Predicting:  89%|████████▉ | 178/200 [00:38<00:04,  4.87it/s]\n",
      "Predicting:  90%|████████▉ | 179/200 [00:39<00:04,  4.37it/s]\n",
      "Predicting:  90%|█████████ | 180/200 [00:39<00:04,  4.33it/s]\n",
      "Predicting:  90%|█████████ | 181/200 [00:39<00:04,  4.26it/s]\n",
      "Predicting:  91%|█████████ | 182/200 [00:40<00:05,  3.13it/s]\n",
      "Predicting:  92%|█████████▏| 183/200 [00:40<00:05,  2.85it/s]\n",
      "Predicting:  92%|█████████▏| 184/200 [00:40<00:06,  2.66it/s]\n",
      "Predicting:  92%|█████████▎| 185/200 [00:41<00:07,  2.10it/s]\n",
      "Predicting:  93%|█████████▎| 186/200 [00:42<00:06,  2.28it/s]\n",
      "Predicting:  94%|█████████▎| 187/200 [00:42<00:06,  1.96it/s]\n",
      "Predicting:  94%|█████████▍| 188/200 [00:43<00:06,  1.91it/s]\n",
      "Predicting:  94%|█████████▍| 189/200 [00:43<00:05,  2.09it/s]\n",
      "Predicting:  95%|█████████▌| 190/200 [00:43<00:04,  2.50it/s]\n",
      "Predicting:  96%|█████████▌| 191/200 [00:44<00:03,  2.95it/s]\n",
      "Predicting:  96%|█████████▌| 192/200 [00:44<00:02,  3.36it/s]\n",
      "Predicting:  96%|█████████▋| 193/200 [00:44<00:01,  3.65it/s]\n",
      "Predicting:  97%|█████████▋| 194/200 [00:44<00:01,  3.85it/s]\n",
      "Predicting:  98%|█████████▊| 195/200 [00:44<00:01,  4.17it/s]\n",
      "Predicting:  98%|█████████▊| 196/200 [00:45<00:00,  4.44it/s]\n",
      "Predicting:  98%|█████████▊| 197/200 [00:45<00:00,  4.09it/s]\n",
      "Predicting:  99%|█████████▉| 198/200 [00:45<00:00,  4.52it/s]\n",
      "Predicting: 100%|█████████▉| 199/200 [00:45<00:00,  4.52it/s]\n",
      "Predicting: 100%|██████████| 200/200 [00:45<00:00,  4.56it/s]\n",
      "Predicting: 100%|██████████| 200/200 [00:45<00:00,  4.35it/s]\n"
     ]
    }
   ],
   "source": [
    "#mode:  running in batch mode\n",
    "!python inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b9093",
   "metadata": {},
   "source": [
    "Instructions for Testing (English)\n",
    "Here are the step-by-step instructions to run and test the API.\n",
    "\n",
    "Ensure you have the required libraries installed in your environment:\n",
    "Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ffd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fastapi uvicorn python-multipart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c5998f",
   "metadata": {},
   "source": [
    "Step 2: Run the Server\n",
    "From the root directory of your project (where api.py is located), run:\n",
    "\n",
    "Bash\n",
    "\n",
    "uvicorn api:app --reload\n",
    "\n",
    "You should see logs indicating \"Starting API and loading models...\".\n",
    "\n",
    "Wait until you see \"Model loaded successfully\".\n",
    "\n",
    "Step 3: Test using Swagger UI (Recommended)\n",
    "FastAPI provides an automatic interactive documentation page.\n",
    "\n",
    "Open your web browser.\n",
    "\n",
    "Go to: http://localhost:8000/docs\n",
    "\n",
    "Click on the POST /predict endpoint bar to expand it.\n",
    "\n",
    "Click the \"Try it out\" button in the top right.\n",
    "\n",
    "Text Field: Enter a sample news headline (e.g., \"Aliens landed in New York\").\n",
    "\n",
    "Image Field: Click \"Choose File\" and select a test image (.jpg or .png).\n",
    "\n",
    "Click the big blue \"Execute\" button.\n",
    "\n",
    "Scroll down to \"Server response\" to see the JSON result (Probability, Label, etc.).\n",
    "\n",
    "Step 4: Test using cURL (Terminal)\n",
    "\n",
    "Step 4: Test using cURL (Terminal)\n",
    "You can also test it via command line:\n",
    "\n",
    "Bash\n",
    "\n",
    "curl -X 'POST' \\\n",
    "  'http://localhost:8000/predict' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: multipart/form-data' \\\n",
    "  -F 'text=This is a breaking news test' \\\n",
    "  -F 'image=@/path/to/your/image.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bfcf32",
   "metadata": {},
   "source": [
    "#Run api inference for pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5e5a88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting API and loading models...\n",
      "2025-12-09 17:36:12 - API - INFO - Using device: cpu\n",
      "2025-12-09 17:36:12 - API - INFO - Loading model from ./src/training/checkpoints/best_model_1.7.pth...\n",
      "2025-12-09 17:36:14 - API - INFO - Model loaded successfully! (Epoch: 4, Val Acc: 0.8420)\n",
      "2025-12-09 17:36:14 - API - INFO - Loading tokenizer: distilbert-base-uncased\n",
      "Model loaded successfully. API is ready.\n",
      "Shutting down API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [11612]\n",
      "INFO:     Waiting for application startup.\n",
      "c:\\Users\\admin\\miniconda3\\envs\\ml-zoomcamp\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\miniconda3\\envs\\ml-zoomcamp\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "INFO:     Application startup complete.\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 8000): [winerror 10048] solo se permite un uso de cada dirección de socket (protocolo/dirección de red/puerto)\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "!python .\\api.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d0ee86",
   "metadata": {},
   "source": [
    "# Build PyTorch model image\n",
    " docker build -t fake-news-api:v1_torch -f Dockerfile-torch ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a60544",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker images\n",
    "\n",
    "IMAGE                                 ID             DISK USAGE   CONTENT SIZE   EXTRA\n",
    "fake-news-api:v1_torch                e4a1cb3143f1       12.1GB             0B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a840f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the container\n",
    "docker run -d -p 8000:8000 --name fake-news-api fake-news-api:v1_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0259b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` \n",
    "to get the most up-to-date weights.\n",
    "  warnings.warn(msg)\n",
    "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
    "100%|██████████| 20.5M/20.5M [00:05<00:00, 3.65MB/s]\n",
    "root@LAPTOP-3IEAPVNM:/mnt/d/workspace/datatalks-club/DataTalksClub/final-project# docker logs fdbd5f6f049e\n",
    "INFO:     Started server process [1]\n",
    "INFO:     Waiting for application startup.\n",
    "Starting API and loading models...\n",
    "2025-12-10 00:44:01 - API - INFO - Using device: cpu\n",
    "2025-12-10 00:44:01 - API - INFO - Loading model from ./src/training/checkpoints/best_model_1.7.pth...\n",
    "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please \n",
    "use 'weights' instead.\n",
    "  warnings.warn(\n",
    "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may \n",
    "be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` \n",
    "to get the most up-to-date weights.\n",
    "  warnings.warn(msg)\n",
    "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
    "100%|██████████| 20.5M/20.5M [00:05<00:00, 3.65MB/s]\n",
    "2025-12-10 00:44:34 - API - INFO - Model loaded successfully! (Epoch: 4, Val Acc: 0.8420)\n",
    "2025-12-10 00:44:34 - API - INFO - Loading tokenizer: distilbert-base-uncased\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "Model loaded successfully. API is ready.\n",
    "INFO:     172.17.0.1:44436 - \"GET / HTTP/1.1\" 404 Not Found\n",
    "INFO:     172.17.0.1:44438 - \"GET / HTTP/1.1\" 404 Not Found\n",
    "INFO:     172.17.0.1:49354 - \"GET / HTTP/1.1\" 404 Not Found\n",
    "root@LAPTOP-3IEAPVNM:/mnt/d/workspace/datatalks-club/DataTalksClub/final-project# docker ps\n",
    "CONTAINER ID   IMAGE                    COMMAND                  CREATED         STATUS         PORTS                                         NAMES\n",
    "fdbd5f6f049e   fake-news-api:v1_torch   \"uvicorn api:app --h…\"   2 minutes ago   Up 2 minutes   0.0.0.0:8000->8000/tcp, [::]:8000->8000/tcp   fake-news-api\n",
    "root@LAPTOP-3IEAPVNM:/mnt/d/workspace/datatalks-club/DataTalksClub/final-project# docker logs fdbd5f6f049e\n",
    "INFO:     Started server process [1]\n",
    "INFO:     Waiting for application startup.\n",
    "Starting API and loading models...\n",
    "2025-12-10 00:44:01 - API - INFO - Using device: cpu\n",
    "2025-12-10 00:44:01 - API - INFO - Loading model from ./src/training/checkpoints/best_model_1.7.pth...\n",
    "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please \n",
    "use 'weights' instead.\n",
    "  warnings.warn(\n",
    "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may \n",
    "be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` \n",
    "to get the most up-to-date weights.\n",
    "  warnings.warn(msg)\n",
    "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
    "100%|██████████| 20.5M/20.5M [00:05<00:00, 3.65MB/s]\n",
    "2025-12-10 00:44:34 - API - INFO - Model loaded successfully! (Epoch: 4, Val Acc: 0.8420)\n",
    "2025-12-10 00:44:34 - API - INFO - Loading tokenizer: distilbert-base-uncased\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "Model loaded successfully. API is ready.\n",
    "INFO:     172.17.0.1:44436 - \"GET / HTTP/1.1\" 404 Not Found\n",
    "INFO:     172.17.0.1:44438 - \"GET / HTTP/1.1\" 404 Not Found\n",
    "INFO:     172.17.0.1:49354 - \"GET / HTTP/1.1\" 404 Not Found\n",
    "INFO:     172.17.0.1:49368 - \"GET / HTTP/1.1\" 404 Not Found\n",
    "root@LAPTOP-3IEAPVNM:/mnt/d/workspace/datatalks-club/DataTalksClub/final-project# docker logs -f fdbd5f6f049e\n",
    "INFO:     Started server process [1]\n",
    "INFO:     Waiting for application startup.\n",
    "Starting API and loading models...\n",
    "2025-12-10 00:44:01 - API - INFO - Using device: cpu\n",
    "2025-12-10 00:44:01 - API - INFO - Loading model from ./src/training/checkpoints/best_model_1.7.pth...\n",
    "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please \n",
    "use 'weights' instead.\n",
    "  warnings.warn(\n",
    "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may \n",
    "be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` \n",
    "to get the most up-to-date weights.\n",
    "  warnings.warn(msg)\n",
    "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
    "100%|██████████| 20.5M/20.5M [00:05<00:00, 3.65MB/s]\n",
    "2025-12-10 00:44:34 - API - INFO - Model loaded successfully! (Epoch: 4, Val Acc: 0.8420)\n",
    "2025-12-10 00:44:34 - API - INFO - Loading tokenizer: distilbert-base-uncased\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "Model loaded successfully. API is ready.\n",
    "INFO:     172.17.0.1:44436 - \"GET / HTTP/1.1\" 404 Not Found\n",
    "INFO:     172.17.0.1:44438 - \"GET / HTTP/1.1\" 404 Not Found\n",
    "INFO:     172.17.0.1:49354 - \"GET / HTTP/1.1\" 404 Not Found\n",
    "INFO:     172.17.0.1:49368 - \"GET / HTTP/1.1\" 404 Not Found\n",
    "INFO:     172.17.0.1:41686 - \"GET / HTTP/1.1\" 404 Not Found\n",
    "INFO:     172.17.0.1:57870 - \"GET /docs HTTP/1.1\" 200 OK\n",
    "INFO:     172.17.0.1:57870 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
    "INFO:     172.17.0.1:45766 - \"GET /docs HTTP/1.1\" 200 OK\n",
    "INFO:     172.17.0.1:45766 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
    "INFO:     172.17.0.1:43066 - \"POST /predict HTTP/1.1\" 200 OK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4fdcbf",
   "metadata": {},
   "source": [
    "#Export pytorch model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b68b180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from src/training/checkpoints/best_model_1.7.pth...\n",
      "Generating ONNX graph...\n",
      "[torch.onnx] Obtain model graph for `OnnxWrapper([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `OnnxWrapper([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 126 of general pattern rewrite rules.\n",
      "✅ Success! Model exported to: multimodal_model.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\ml-zoomcamp\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\miniconda3\\envs\\ml-zoomcamp\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "d:\\workspace\\datatalks-club\\DataTalksClub\\final-project\\export_onnx.py:73: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from src/training/checkpoints/best_model_1.7.pth...\n",
      "Generating ONNX graph...\n",
      "[torch.onnx] Obtain model graph for `OnnxWrapper([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `OnnxWrapper([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 126 of general pattern rewrite rules.\n",
      "✅ Success! Model exported to: multimodal_model.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\ml-zoomcamp\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\miniconda3\\envs\\ml-zoomcamp\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "d:\\workspace\\datatalks-club\\DataTalksClub\\final-project\\export_onnx.py:73: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n"
     ]
    }
   ],
   "source": [
    "!python export_onnx.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02ef80f",
   "metadata": {},
   "source": [
    "# Build onnx model docker image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2816f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    " docker build -t fake-news-api:v1_onnx -f Dockerfile-onxx .\n",
    "\n",
    " docker images\n",
    "\n",
    "IMAGE                                 ID             DISK USAGE   CONTENT SIZE   EXTRA\n",
    "fake-news-api:v1_onnx                 914ca79bb7a3        754MB             0B    U "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab02f937",
   "metadata": {},
   "source": [
    "# Running onnx model docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a00b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    " docker run -d -p 8000:8000 --name fake-news-api fake-news-api:v1_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3632b3",
   "metadata": {},
   "source": [
    "# docker ps\n",
    "CONTAINER ID   IMAGE                   COMMAND                  CREATED          STATUS          PORTS                                         NAMES\n",
    "e0c0fdb7bb04   fake-news-api:v1_onnx   \"uvicorn api-onnx:ap…\"   14 seconds ago   Up 13 seconds   0.0.0.0:8000->8000/tcp, [::]:8000->8000/tcp   fake-news-api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f170b6",
   "metadata": {},
   "source": [
    "docker logs -f e0c0fdb7bb04\n",
    "2025-12-10 02:53:50.973359585 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\n",
    "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
    "INFO:     Started server process [1]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "Starting API and loading models...\n",
    "Loading ONNX model from: multimodal_model.onnx\n",
    "Model loaded successfully. API is ready.\n",
    "INFO:     172.17.0.1:49728 - \"GET / HTTP/1.1\" 404 Not Found\n",
    "INFO:     172.17.0.1:49728 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
    "INFO:     172.17.0.1:49724 - \"GET /docs HTTP/1.1\" 200 OK\n",
    "INFO:     172.17.0.1:49724 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
    "INFO:     172.17.0.1:36638 - \"POST /predict HTTP/1.1\" 200 OK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc8781",
   "metadata": {},
   "source": [
    "# Deployment in Minikube for ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cac808",
   "metadata": {},
   "outputs": [],
   "source": [
    "minikube status\n",
    "minikube start\n",
    "\n",
    "cat /etc/passwd | cut -d: -f1\n",
    "getent passwd | awk -F: '$3 >= 1000 && $1 != \"nobody\" {print $1}'\n",
    "\n",
    "\n",
    "for user in $(getent passwd | awk -F: '$3 >= 1000 {print $1}'); do\n",
    "    if groups $user | grep -q k8suser; then\n",
    "        echo \"$user has docker access\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sudo -u miuser -i which kubectl\n",
    "\n",
    "\n",
    "sudo -u miuser -i minikube start\n",
    "\n",
    "\n",
    " sudo su - miuser\n",
    "\n",
    "\n",
    " minikube image load fake-news-api:v1_onnx\n",
    "\n",
    "minikube ssh -- docker images\n",
    "\n",
    " kubectl apply -f k8s-onnx-deployment.yaml\n",
    "\n",
    "\n",
    "kubectl get all\n",
    "NAME                                        READY   STATUS    RESTARTS   AGE\n",
    "pod/fake-news-deployment-6479788fb4-n2kpb   1/1     Running   0          81s\n",
    "\n",
    "NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\n",
    "service/fake-news-service   NodePort    10.110.221.161   <none>        80:31018/TCP   81s\n",
    "service/kubernetes          ClusterIP   10.96.0.1        <none>        443/TCP        29d\n",
    "\n",
    "NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "deployment.apps/fake-news-deployment   1/1     1            1           81s\n",
    "\n",
    "NAME                                              DESIRED   CURRENT   READY   AGE    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "kubectl port-forward service/fake-news-service 8000:80\n",
    "\n",
    "\n",
    "minikube service fake-news-service --url\n",
    "\n",
    "http://192.168.49.2:31018\n",
    "\n",
    "minikube tunnel\n",
    "\n",
    "\n",
    "# Redirige el puerto 8000 de tu PC al 8000 del servicio\n",
    "kubectl port-forward service/fake-news-service 8000:80\n",
    "\n",
    "miuser@LAPTOP-3IEAPVNM:/mnt/d/workspace/datatalks-club/DataTalksClub/final-project$ kubectl port-forward service/fake-news-service 8000:80\n",
    "Forwarding from 127.0.0.1:8000 -> 8000\n",
    "Forwarding from [::1]:8000 -> 8000\n",
    "Handling connection for 8000\n",
    "Handling connection for 8000\n",
    "Handling connection for 8000\n",
    "\n",
    "\n",
    "\n",
    "curl -X 'POST' \\\n",
    " 'http:>   'http://localhost:8000/predict' \\\n",
    ">   -H 'accept: application/json' \\\n",
    ">   -H 'Content-Type: multipart/form-data' \\\n",
    "  -F 't>   -F 'text=This frog sitting on a light' \\\n",
    ">   -F 'image=@/mnt/d/workspace/datatalks-club/DataTalksClub/final-project/data/images/test/c7jxj5.jpg'\n",
    "\n",
    "{\"filename\":\"c7jxj5.jpg\",\"prediction\":\"FAKE\",\"probability_fake\":0.6504932641983032,\"confidence\":0.6504932641983032,\"label_id\":1}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
