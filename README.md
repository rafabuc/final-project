# Multimodal Fake News Detection System

A deep learning system that combines visual and textual information to detect fake news using a multimodal neural network architecture with MLflow tracking, Docker deployment, and Kubernetes orchestration.

---

## Table of Contents

1. [The Problem](#the-problem)
2. [The Solution](#the-solution)
3. [Model Architecture](#model-architecture)
4. [Results](#-results)
5. [Project Structure](#project-structure)
6. [Main Source Files Description](#main-source-files-description)
7. [Key Components](#key-components)
8. [Installation](#installation)
9. [Configuration](#configuration)
10. [Training](#training)
11. [Inference](#inference)
12. [Deployment](#deployment)
13. [Data and Model Versioning](#data-and-model-versioning)
14. [Architecture Details](#architecture-details)
15. [Performance](#performance)
16. [Troubleshooting](#troubleshooting)
17. [Advanced Usage](#advanced-usage)
18. [File Size Guidelines](#file-size-guidelines)
19. [Storage Options](#storage-options)
20. [Best Practices](#best-practices)
21. [References](#references)
22. [Contact & Support](#contact--support)
23. [Acknowledgments](#acknowledgments)
24. [Quick Start Commands](#quick-start-commands)

---

## The Problem

In the modern digital landscape, misinformation often manifests not just as fake text or manipulated images, but as a **semantic conflict** between the two. A real image can be captioned with a lie, or a real news story can be illustrated with a misleading image.

Traditional unimodal models (Text-only or Image-only) fail to catch these subtleties:
* **Text Models** miss visual context.
* **Image Models** cannot verify the factual claims in the caption.

---

## The Solution

This system treats the detection task as a **Multimodal Classification Problem**. It processes the visual and textual streams independently to extract high-level feature representations, which are then fused to learn the cross-modal relationship. If the image and text features do not align semantically, the model classifies the content as **FAKE**.

### Key Features

- **Multimodal Architecture**: Combines EfficientNet-B0 (vision) + DistilBERT (text)
- **Late Fusion Strategy**: Independent feature learning with cross-modal fusion
- **MLflow Integration**: Complete experiment tracking and model versioning
- **Production Ready**: Docker containers, Kubernetes deployment, ONNX export
- **Data Versioning**: DVC for datasets, Git LFS for model checkpoints
- **Hydra Configuration**: Flexible, composable configuration management

---

## Model Architecture

### High-Level Design

```
                            INPUT DATA
                                |
                    +-----------+-----------+
                    |                       |
                  IMAGE                   TEXT
                    |                       |
            +-------v-------+       +-------v--------+
            | EfficientNet  |       | DistilBERT     |
            | (Pre-trained) |       | (Pre-trained)  |
            +-------+-------+       +-------+--------+
                    |                       |
            [1280 features]         [768 features]
                    |                       |
            +-------v-------+       +-------v--------+
            | Vision FC     |       | Text FC        |
            | 1280 -> 512   |       | 768 -> 512     |
            +-------+-------+       +-------+--------+
                    |                       |
                    +----------+------------+
                               |
                        FUSION LAYER
                               |
                    +----------v-----------+
                    |  Concatenate (1024)  |
                    +----------+-----------+
                               |
                    +----------v-----------+
                    |   Fusion Network     |
                    |   1024 -> 256 -> 1   |
                    +----------+-----------+
                               |
                          Sigmoid
                               |
                        OUTPUT (0-1)
                    [0: Real, 1: Fake]
```

---

> **ðŸ“– For detailed architecture documentation, see [ARCHITECTURE.md](ARCHITECTURE.md)**
>
> This document covers:
> - Component specifications (Vision Branch, Text Branch, Fusion Layer)
> - Design decisions and architectural choices
> - Fusion strategies (why late fusion vs early fusion)
> - Implementation details and parameter counts

---

### Architecture Components

#### 1. Vision Branch (The "Eye")
* **Backbone:** EfficientNet-B0 (Pretrained on ImageNet)
* **Input:** RGB Images resized to 224x224 and normalized
* **Processing:** Features extracted from Global Average Pooling (1280-dim) â†’ 512-dim embedding space

#### 2. Text Branch (The "Brain")
* **Backbone:** DistilBERT-base-uncased (Pretrained on Wikipedia)
* **Input:** Tokenized text (padded to 128 tokens)
* **Processing:** [CLS] token embedding (768-dim) â†’ 512-dim embedding space

#### 3. Fusion & Classification
* **Fusion:** Visual and text embeddings concatenated (512+512=1024)
* **Classifier:** Fully Connected Network (Dense Layers + ReLU + Dropout) â†’ single logit for binary classification

### Training Strategy

To prevent "Catastrophic Forgetting" of the pretrained weights, we employed a **Two-Stage Transfer Learning** strategy:

1.  **Phase 1 (Head Warming):** The backbones (EfficientNet/DistilBERT) were frozen. Only the fusion head was trained to align the random weights with the pretrained features. Parameters were frozen for the backbones. 
freeze_vision: true 
freeze_text: true 
learning_rate: 0.0001
Best Model: src/trainig/checkpoints/best_model_1.6.pth

2.  **Phase 2 (Fine-Tuning):** The entire model was unfrozen and trained with a very low learning rate (`2e-5`) to fine-tune the feature extractors for fake news patterns.
freeze_vision: false 
freeze_text: false 
learning_rate: 0.00005 
Best Model: src/trainig/checkpoints/best_model_1.7.pth

## ðŸ“Š Results

| Metric | Phase 1 (Frozen) | Phase 2 (Fine-Tuned) |
| :--- | :---: | :---: |
| **Validation Accuracy** | ~74.1% | **84.2%** |
| **Training Accuracy** | ~72.0% | **92.6%** |


---

## Project Structure

```
final-project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset.py           # MultimodalDataset class
â”‚   â”‚   â”œâ”€â”€ load_data.py         # Data loading utilities
â”‚   â”‚   â””â”€â”€ transforms.py        # Image transformations
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ multimodal_net.py    # MultimodalNet architecture
â”‚   â”‚   â”œâ”€â”€ vision_branch.py     # EfficientNet branch
â”‚   â”‚   â””â”€â”€ text_branch.py       # DistilBERT branch
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train.py             # Main training script
â”‚   â”‚   â”œâ”€â”€ trainer.py           # MultimodalTrainer with MLflow
â”‚   â”‚   â”œâ”€â”€ visualization.py     # Training visualization
â”‚   â”‚   â”œâ”€â”€ mlflow.db            # MLflow database
â”‚   â”‚   â”œâ”€â”€ checkpoints/         # Model checkpoints (Git LFS)
â”‚   â”‚   â””â”€â”€ mlruns/              # MLflow experiment tracking
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logging.py           # Logger setup
â”‚       â””â”€â”€ metrics.py           # Performance metrics
â”‚
â”œâ”€â”€ configs/                     # Hydra configuration files
â”‚   â”œâ”€â”€ config.yaml             # Main config
â”‚   â”œâ”€â”€ config_inference.yaml   # Inference config
â”‚   â”œâ”€â”€ model/                  # Model configs
â”‚   â”œâ”€â”€ data/                   # Data configs
â”‚   â”œâ”€â”€ training/               # Training configs
â”‚   â””â”€â”€ inference/              # Inference configs
â”‚
â”œâ”€â”€ data/                        # Dataset directory (DVC tracked)
â”‚   â”œâ”€â”€ images/                 # Image files
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ val/
â”‚   â”‚   â””â”€â”€ test/
â”‚   â”œâ”€â”€ train.csv               # Training data
â”‚   â”œâ”€â”€ val.csv                 # Validation data
â”‚   â”œâ”€â”€ test.csv                # Test data
â”‚   â””â”€â”€ test-predictions.csv    # Test predictions (generated by inference.py in batch mode)
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter notebooks
â”‚   â””â”€â”€ inference.ipynb         # Inference examples and API usage
â”‚
â”‚
â”œâ”€â”€ mlruns/                      # MLflow experiment tracking (root level)
â”‚
â”œâ”€â”€ outputs/                     # Hydra output directory

â”œâ”€â”€ inference.py                 # Inference script (Hydra and PyTorch)
â”œâ”€â”€ api.py                       # FastAPI REST API (PyTorch)
â”œâ”€â”€ inference-onnx.py            # Inference script (Hydra and ONNX Runtime)
â”œâ”€â”€ api-onnx.py                  # FastAPI REST API (ONNX Runtime)
â”œâ”€â”€ export_onnx.py               # PyTorch â†’ ONNX converter
â”œâ”€â”€ multimodal_model.onnx        # ONNX model Tracked with DVC
â”œâ”€â”€ multimodal_model.onnx.data   # ONNX model Tracked with DVC
â”‚
â”œâ”€â”€ Dockerfile-torch             # Docker (PyTorch)
â”œâ”€â”€ Dockerfile-onnx              # Docker (ONNX Runtime)
â”œâ”€â”€ k8s-onnx-deployment.yaml     # Kubernetes deployment (ONNX Runtime)
â”‚
â”œâ”€â”€ requirements-dev.txt         # Development dependencies
â”œâ”€â”€ requirements-deploy-torch.txt # PyTorch deployment (to build docker pytorch image)
â”œâ”€â”€ requirements-deploy-onnx.txt  # ONNX deployment (to build docker onnx image)
â”‚
â”œâ”€â”€ .gitignore                   # Git ignore file
â”œâ”€â”€ .gitattributes               # Git attributes file
â”œâ”€â”€ .dvc                         # DVC configuration
â”œâ”€â”€ .dvc-ignore                  # DVC ignore file
â”‚
â”œâ”€â”€ architecture_diagram.png     # Architecture diagram
â”œâ”€â”€ ARCHITECTURE.md              # Detailed architecture documentation
â”œâ”€â”€ INFERENCE_INSTRUCTIONS.md    # Inference instructions

â””â”€â”€ README.md                    # This file
```

---

## Main Source Files Description

### Data Module (`src/data/`)

#### `dataset.py`
Custom PyTorch Dataset implementation for multimodal fake news detection.
- **Class:** `MultimodalDataset`
- **Features:**
  - Loads image-text pairs from CSV files
  - Handles image preprocessing and text tokenization
  - Supports flexible column naming
  - Returns batches with images, tokenized text, and labels
- **Usage:** Training and inference data loading

#### `load_data.py`
Data loading utilities and helper functions.
- CSV file processing
- Data validation and error handling
- Path resolution for image files

#### `transforms.py`
Image transformation pipelines using torchvision.
- **Training transforms:** RandomHorizontalFlip, RandomRotation, ColorJitter, normalization
- **Validation/Test transforms:** Resize, normalization
- ImageNet statistics normalization
- Returns transforms for different modes (train/val/test)

---

### Models Module (`src/models/`)

#### `multimodal_net.py`
Main multimodal architecture with late fusion strategy.
- **Class:** `MultimodalNet`
- **Architecture:**
  - Combines vision and text branches
  - Late fusion: concatenates embeddings (1024-dim)
  - Fusion network: 1024 â†’ 256 â†’ 128 â†’ 1
  - Dropout layers for regularization
- **Parameters:**
  - `embedding_dim`: 512 (default)
  - `fusion_hidden_dim`: 256 (default)
  - `dropout_rate`: 0.4 (default)
  - `freeze_vision`, `freeze_text`: Transfer learning control

#### `vision_branch.py`
Vision encoder using EfficientNet-B0.
- **Class:** `VisionBranch`
- **Architecture:**
  - Pretrained EfficientNet-B0 backbone (ImageNet)
  - Global Average Pooling: 1280-dim features
  - Projection head: 1280 â†’ 512 â†’ 512
  - ReLU activation and dropout
- **Features:**
  - Optional backbone freezing for transfer learning
  - ~4M parameters

#### `text_branch.py`
Text encoder using DistilBERT.
- **Class:** `TextBranch`
- **Architecture:**
  - Pretrained DistilBERT-base-uncased
  - [CLS] token embedding: 768-dim
  - Projection head: 768 â†’ 512 â†’ 512
  - ReLU activation and dropout
- **Features:**
  - Optional backbone freezing
  - ~66M parameters
  - Handles input_ids and attention_mask

---

### Training Module (`src/training/`)

#### `train.py`
Main training script with Hydra configuration.
- **Functions:**
  - Loads configuration via Hydra decorators
  - Initializes dataloaders, model, optimizer
  - Sets up MLflow experiment tracking
  - Executes training loop via MultimodalTrainer
  - Saves best model checkpoints
- **Features:**
  - Two-phase training (frozen â†’ fine-tuned)
  - OneCycleLR learning rate scheduling
  - Automatic device detection (CPU/GPU)

#### `trainer.py`
Training loop implementation with MLflow integration.
- **Class:** `MultimodalTrainer`
- **Features:**
  - Training and validation epochs
  - Gradient clipping and mixed precision support
  - MLflow logging (metrics, parameters, artifacts)
  - Model checkpointing (best model based on validation loss)
  - Progress tracking with tqdm
  - Comprehensive metric calculation

#### `visualization.py`
Training visualization utilities.
- Plotting functions for loss curves
- Metric visualization
- Confusion matrix plotting
- ROC curve generation

---

### Utils Module (`src/utils/`)

#### `logging.py`
Logging configuration and setup.
- **Function:** `setup_logger()`
- **Features:**
  - Configurable log levels
  - File and console handlers
  - Formatted output with timestamps
  - Logger name customization

#### `metrics.py`
Performance metrics calculation.
- **Metrics implemented:**
  - Accuracy
  - Precision, Recall, F1-Score
  - ROC-AUC
  - Confusion Matrix
- **Functions:**
  - `calculate_metrics()`: Computes all metrics from predictions
  - Binary classification metric utilities

---

## Key Components

### 1. Data Pipeline
- **MultimodalDataset**: Custom PyTorch Dataset that loads images and text
- **Image Transforms**: Data augmentation for training (RandomHorizontalFlip, RandomRotation, ColorJitter) and validation
- **Text Tokenization**: DistilBERT tokenizer with max_length=128

### 2. Model Architecture
- **Vision Encoder**: ResNet-50 (pre-trained on ImageNet)
  - Output: 2048-dim features -> 512-dim embeddings
- **Text Encoder**: DistilBERT (pre-trained)
  - Output: 768-dim features -> 512-dim embeddings
- **Fusion Network**:
  - Concatenates vision and text embeddings (1024-dim)
  - Two fully connected layers (1024 -> 256 -> 1)
  - Dropout for regularization
  - Sigmoid activation for binary classification

### 3. Training Pipeline
- **Loss Function**: Binary Cross-Entropy Loss (BCELoss)
- **Optimizer**: Adam or AdamW with weight decay
- **Learning Rate Scheduler**: Cosine Annealing
- **Gradient Clipping**: max_norm=1.0 for stability
- **MLflow Tracking**: Logs hyperparameters, metrics, and model artifacts

### 4. Metrics
- Accuracy
- Precision, Recall, F1-Score
- ROC-AUC
- Confusion Matrix


---

## Installation

### Prerequisites

- Python 3.11
- CUDA 11.0+ (for GPU support)
- 8GB+ RAM
- 4GB+ GPU memory (recommended)
- Git, 
- DVC (for data versioning)

### Step 1: Clone Repository

```bash

# Clone repository
git clone <repository-url>
cd final-project
```

### Step 2: Download Data and Models


This project uses DVC (Data Version Control) to manage large datasets and artifacts that cannot be stored directly on GitHub.

### Prerequisites
```bash
pip install dvc[s3]
```

### Setup and Download

1. **Configure DVC remote:**
```bash
dvc remote add -d myremote s3://q-reg/artifact-final-project
```

2. **Configure anonymous access (public bucket):**
```bash
dvc remote modify myremote anon true
```

3. **Download the data:**
```bash
# Download all files
dvc pull

# Or download specific files
dvc pull multimodal_model.onnx.dvc
dvc pull multimodal_model.onnx.data.dvc
dvc pull data.dvc
dvc pull src/training/checkpoints.dvc
dvc pull src/training/outputs.dvc
```

### Data Structure

After running `dvc pull`, you will have:
```
.
â”œâ”€â”€ data/
    â””â”€â”€ images/train/     # Image dataset
    â””â”€â”€ images/test/      # Image dataset
    â””â”€â”€ images/val/       # Image dataset
    â””â”€â”€ train.csv         # Training dataset
    â””â”€â”€ test.csv          # Test dataset
    â””â”€â”€ val.csv           # Validation dataset
    â””â”€â”€ test-predictions.csv  # Test predictions (generated by inference.py in batch mode)


â”œâ”€â”€ multimodal_model.onnx          # ONNX model file
â”œâ”€â”€ multimodal_model.onnx.data     # ONNX model data
â””â”€â”€ src/
    â””â”€â”€ training/
        â”œâ”€â”€ checkpoints/           # Training checkpoints
            â”œâ”€â”€ best_model_1.6.pth      # Best model checkpoint in phase 1 (freezing vision encoder)

            â”œâ”€â”€ best_model_1.7.pth      # Best model checkpoint in phase 2 (fine-tuning vision   encoder, takes best_model_1.6.pth as initial checkpoint) FINAL MODEL
            
            â”œâ”€â”€ best_model_1.8.pth      # Improved model checkpoint in phase 2 (fine-tuning vision encoder takes best_model_1.7.pth as initial checkpoint)
            
        â””â”€â”€ outputs/              # Training outputs
```

### Troubleshooting

If you encounter issues downloading:
```bash
# Verify remote configuration
dvc remote list

# Check connection to remote
dvc remote list --verbose

# Force re-download
dvc pull --force

### Step 3: Create Virtual Environment

```bash
# Using conda (recommended)
conda create -n fakenews python=3.11
conda activate fakenews

# OR using venv
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Step 4: Install Dependencies

```bash
# Development (includes all dependencies)
pip install -r requirements-dev.txt


```

---

## Configuration

The project uses **Hydra** for hierarchical configuration management.

### ðŸ“š Complete Configuration Documentation

**âš ï¸ IMPORTANT:** Before training or modifying the model, please review the comprehensive configuration guide:

**âž¡ï¸ [ðŸ“– CONFIG.md - Complete Configuration Reference](CONFIG.md)**

This essential guide covers:
- âœ… **Model Loading & Versioning** - How to load checkpoints and version models (`load_model`, `best_model_name`, `version`)
- âœ… **Transfer Learning Controls** - Freeze/unfreeze layers for speed vs accuracy (`freeze_vision`, `freeze_text`)
- âœ… **Training Parameters** - Learning rate, dropout, optimizer settings and their impact
- âœ… **Performance Impact Analysis** - How each parameter affects training speed, GPU memory, and accuracy
- âœ… **Workflow Examples** - Full fine-tuning, frozen backbones, incremental training scenarios
- âœ… **Best Practices** - Preventing catastrophic forgetting, hyperparameter tuning strategies
- âœ… **Command Line Overrides** - How to override any configuration from the command line
- âœ… **Troubleshooting** - Common configuration issues and solutions

### âš¡ Quick Configuration Reference

**Critical Variables for Incremental Training:**

```yaml
# configs/model/multimodal.yaml
load_model: true                    # â† Load previous checkpoint before training
best_model_name: best_model_1.7.pth # â† Source checkpoint file to start from
version: 1.8                        # â† New version identifier for this training run

freeze_vision: false                # â† Fine-tune vision backbone (EfficientNet)
freeze_text: false                  # â† Fine-tune text backbone (DistilBERT)
dropout_rate: 0.3                   # â† Regularization strength (0.3-0.5)

# configs/training/default.yaml
learning_rate: 0.00002              # â† Low LR for fine-tuning (2e-5)
optimizer: adamw                    # â† Recommended for transfer learning
use_scheduler: true                 # â† Dynamic LR with OneCycleLR
num_epochs: 5                       # â† Training epochs
batch_size: 32                      # â† Samples per batch
```

**Performance Impact Quick Guide:**
- `freeze_vision=true` â†’ âš¡ **2x faster training**, ðŸ’¾ **40% less GPU memory**, ðŸ“Š Lower accuracy
- `freeze_text=true` â†’ âš¡ **2.5x faster training**, ðŸ’¾ **50% less GPU memory**, ðŸ“Š Lower accuracy
- `learning_rate â‰¤ 2e-5` â†’ ðŸ”’ **Required for stable fine-tuning** (prevents catastrophic forgetting)
- `dropout_rate: 0.5` â†’ ðŸ“ˆ Better generalization, lower training accuracy
- `dropout_rate: 0.3` â†’ ðŸ“Š Higher training accuracy, possible overfitting

**ðŸ‘‰ See [CONFIG.md](CONFIG.md) for complete parameter documentation with code references and examples.**

### Configuration Files Structure

```
configs/
â”œâ”€â”€ config.yaml              # Main training configuration
â”œâ”€â”€ config_inference.yaml    # Inference configuration
â”œâ”€â”€ model/
â”‚   â””â”€â”€ multimodal.yaml     # Model architecture & checkpointing â­
â”œâ”€â”€ data/
â”‚   â””â”€â”€ fakeddit.yaml       # Dataset paths & preprocessing
â”œâ”€â”€ training/
â”‚   â””â”€â”€ default.yaml        # Training hyperparameters â­
â””â”€â”€ inference/
    â””â”€â”€ default.yaml        # Inference settings
```

---

## Training

### Start Training

```bash
# Basic training
python src/training/train.py

```

### Monitor with MLflow

```bash
# In a separate terminal
cd src/training
mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5000

# Open browser: http://localhost:5000
```

### Training Pipeline

**Components:**

1. **Loss Function:** Binary Cross-Entropy (BCE)
2. **Optimizer:** AdamW with weight decay
3. **Learning Rate Scheduler:** OneCycleLR with cosine annealing
4. **Gradient Clipping:** max_norm=1.0 for stability

**MLflow Tracking:**
- Training/Validation Loss, Accuracy, Precision, Recall, F1, AUC-ROC
- Model checkpoints and configuration files
- Learning rate progression

---

## Inference

---

> **ðŸ“– Complete Inference Guide: [INFERENCE_INSTRUCTIONS.md](INFERENCE_INSTRUCTIONS.md)**
>
> Comprehensive documentation covering:
> - ðŸ” **Single Sample Prediction** - How to predict on individual image-text pairs
> - ðŸ“Š **Batch Prediction** - Processing multiple samples from CSV files
> - ðŸš€ **API Usage** - FastAPI endpoints (PyTorch and ONNX versions)
> - âš¡ **ONNX Inference** - High-performance production deployment
> - ðŸ› **Troubleshooting** - Common issues and solutions
> - ðŸ’¡ **Best Practices** - Tips for optimal inference performance

---

The inference system supports both single sample and batch predictions using Hydra configuration.

### Single Sample Prediction

```bash
python inference.py \
  inference.mode=single \
  inference.single.image_path=data/images/test.jpg \
  inference.single.text="Breaking news article text here"
```

**Example Output:**
```
Prediction: REAL
Probability (Fake): 0.1527
Confidence: 0.8473
```

### Batch Prediction

```bash
python inference.py \
  inference.mode=batch \
  inference.batch.csv_file=data/test.csv \
  inference.batch.output_file=predictions.csv
```

**CSV Format:**
```csv
text_content,image_path,label
"News text...",path/to/img1.jpg,0
"Fake news...",path/to/img2.jpg,1
```

**Output:** `predictions.csv` with columns:
- `prediction`: "FAKE" or "REAL"
- `label`: 1 (FAKE) or 0 (REAL)
- `probability`: Raw probability of being fake (0-1)
- `confidence`: Confidence in prediction (0-1)



---

## Deployment

### 1. REST API (Development)

#### Install Dependencies

```bash
pip install fastapi uvicorn python-multipart
```

#### Run API Server

```bash
# Start server with auto-reload
uvicorn api:app --reload

# Server runs on http://localhost:8000
```

#### Test via Swagger UI

1. Open: `http://localhost:8000/docs`
2. Click POST `/predict` endpoint
3. Click "Try it out"
4. Upload image and enter text
5. Click "Execute"
6. See test.docx for example output
#### Test via cURL

```bash
curl -X 'POST' \
  'http://localhost:8000/predict' \
  -H 'accept: application/json' \
  -H 'Content-Type: multipart/form-data' \
  -F 'text=Breaking news test' \
  -F 'image=@/path/to/image.jpg'
```

**Response:**
```json
{
  "filename": "test_image.jpg",
  "prediction": "FAKE",
  "probability_fake": 0.6505,
  "confidence": 0.6505,
  "label_id": 1
}
```

---

### 2. Docker Deployment

#### Option A: PyTorch Version (Full Features)

```bash
# Build image (~12.1GB)
docker build -t fake-news-api:v1_torch -f Dockerfile-torch .

# Run container
docker run -d -p 8000:8000 --name fake-news-api fake-news-api:v1_torch

# Monitor logs
docker logs -f fake-news-api

# Test API
curl -X POST http://localhost:8000/predict \
  -F 'text=Test news' \
  -F 'image=@test.jpg'
```

#### Option B: ONNX Version (Production, Lightweight)

**Benefits:**
- 94% smaller (754MB vs 12.1GB)
- Faster startup and inference
- Lower memory footprint

**Export to ONNX:**

```bash
# Convert PyTorch model to ONNX
python export_onnx.py

# Output: multimodal_model.onnx
```

**Build and Run:**

```bash
# Build ONNX image (~754MB)
docker build -t fake-news-api:v1_onnx -f Dockerfile-onnx .

# Run container
docker run -d -p 8000:8000 --name fake-news-api fake-news-api:v1_onnx

# Same API interface as PyTorch version
```

---

### 3. Kubernetes Deployment

#### Prerequisites

```bash
# Start Minikube
minikube start
minikube status
```

#### Load Image into Minikube

```bash
# Transfer Docker image to Minikube
minikube image load fake-news-api:v1_onnx

# Verify
minikube ssh -- docker images
```

#### Deploy to Kubernetes

```bash
# Apply deployment
kubectl apply -f k8s-onnx-deployment.yaml

# Check status
kubectl get all

# Expected output:
# pod/fake-news-deployment-xxx   1/1   Running
# service/fake-news-service      NodePort   10.x.x.x   <none>   80:31018/TCP
```

#### Access the Service

**Option 1: Port Forwarding**
```bash
kubectl port-forward service/fake-news-service 8000:80

# Access: http://localhost:8000
```

**Option 2: Minikube Service URL**
```bash
minikube service fake-news-service --url

# Returns: http://192.168.49.2:31018
```

**Option 3: Minikube Tunnel**
```bash
minikube tunnel

# Provides external IP access (requires sudo)
```

#### Test Deployment

```bash
curl -X POST http://localhost:8000/predict \
  -F 'text=This frog sitting on a light' \
  -F 'image=@test.jpg'
```

---

### Deployment Comparison

| Method | Use Case | Size | Speed | Complexity |
|--------|----------|------|-------|------------|
| **Local Script** | Development, testing | N/A | Fast | Low |
| **API (Local)** | Development, testing | N/A | Fast | Low |
| **Docker (PyTorch)** | Full features, debugging | 12.1GB | Medium | Medium |
| **Docker (ONNX)** | Production, efficiency | 754MB | Fast | Medium |
| **Kubernetes** | Scalability, production | 754MB | Fast | High |

---

## Data and Model Versioning

### Strategy Overview

| Artifact Type | Location | Size | Strategy | Tool |
|---------------|----------|------|----------|------|
| **Training Images** | `data/images/` | Large (GB) | Remote Storage | DVC |
| **Model Checkpoints** | `src/training/checkpoints/` | Medium (100MB-2GB) | Version Control | Git LFS |
| **MLflow Runs** | `src/training/mlruns/` | Growing | Ignored + Backup | .gitignore |
| **Final Models** | Releases | Medium-Large | GitHub Releases | Git Tags |

---

### DVC for Training Images

**Why DVC?**
- Designed for large datasets
- Works with cloud storage (S3, Google Drive, Azure)
- Lightweight metadata in Git
- Easy sharing and reproduction

#### Initial Setup

```bash
# Install DVC
pip install dvc dvc-gdrive

# Initialize DVC
dvc init

# Configure remote storage (Google Drive)
dvc remote add -d gdrive gdrive://YOUR_GOOGLE_DRIVE_FOLDER_ID

# Track images
dvc add data/images

# Commit metadata
git add data/images.dvc .gitignore .dvc/config
git commit -m "Add training images to DVC tracking"

# Push data to remote
dvc push

# Push metadata to Git
git push
```

#### Daily Workflow

```bash
# 1. Add/modify images in data/images/
# 2. Update DVC tracking
dvc add data/images

# 3. Commit changes
git add data/images.dvc
git commit -m "Update training dataset - added 500 new images"

# 4. Push to both remotes
dvc push  # Data to DVC remote
git push  # Metadata to GitHub
```

#### Download Data (New User)

```bash

# Pull data from DVC
dvc pull

# Verify
ls data/images/
```

---



#### Daily Workflow



---

### Quick Reference Commands

#### DVC Commands

```bash
dvc add data/images          # Track data with DVC
dvc push                     # Upload to remote
dvc pull                     # Download from remote
dvc status                   # Check DVC status
dvc checkout                 # Restore data versions
```

#### Git LFS Commands

```bash
git lfs track "*.pth"        # Track file pattern
git lfs ls-files             # List LFS files
git lfs pull                 # Download LFS files
git lfs prune                # Clean LFS cache
```

#### Version Control

```bash
# Checkout specific data version
git checkout v1.0
dvc checkout

```

---

## Architecture Details

---

> **ðŸ“– For comprehensive architecture documentation, see [ARCHITECTURE.md](ARCHITECTURE.md)**

---

### Component Specifications

#### 1. Vision Branch (`src/models/vision_branch.py`)

```python
EfficientNet-B0 (pretrained)
â”œâ”€â”€ Input: [B, 3, 224, 224]
â”œâ”€â”€ Features: [B, 1280]
â””â”€â”€ Projection:
    â”œâ”€â”€ Linear(1280, 512)
    â”œâ”€â”€ ReLU
    â”œâ”€â”€ Dropout(0.3)
    â””â”€â”€ Linear(512, 512)
```

**Features:**
- Pretrained on ImageNet
- Efficient architecture (~4M parameters)
- Optional backbone freezing
- Custom projection head for embedding normalization

#### 2. Text Branch (`src/models/text_branch.py`)

```python
DistilBERT (pretrained)
â”œâ”€â”€ Input: [B, seq_len] (token IDs + attention mask)
â”œâ”€â”€ CLS Token: [B, 768]
â””â”€â”€ Projection:
    â”œâ”€â”€ Linear(768, 512)
    â”œâ”€â”€ ReLU
    â”œâ”€â”€ Dropout(0.3)
    â””â”€â”€ Linear(512, 512)
```

**Features:**
- Distilled BERT (40% smaller, 60% faster)
- Uses [CLS] token representation
- ~66M parameters
- Optional backbone freezing

#### 3. Fusion Layer (`src/models/multimodal_net.py`)

```python
Late Fusion
â”œâ”€â”€ Concatenate: [B, 1024] (512 + 512)
â”œâ”€â”€ Linear(1024, 256)
â”œâ”€â”€ ReLU
â”œâ”€â”€ Dropout(0.4)
â”œâ”€â”€ Linear(256, 128)
â”œâ”€â”€ ReLU
â”œâ”€â”€ Dropout(0.2)
â”œâ”€â”€ Linear(128, 1)
â””â”€â”€ Sigmoid
```

**Design Choice: Late Fusion**
- Allows each modality to learn independently
- Easier to debug and interpret
- Better for imbalanced modalities
- Simpler than cross-attention alternatives

---

### Data Pipeline

**Image Processing:**
```python
Image.open() â†’ RGB conversion
â†’ Resize(224x224)
â†’ Data Augmentation (train only):
  - Random horizontal flip
  - Random rotation (Â±15Â°)
  - Color jitter
â†’ ToTensor
â†’ Normalize(ImageNet stats)
```

**Text Processing:**
```python
Raw text
â†’ DistilBertTokenizer
â†’ Add [CLS], [SEP] tokens
â†’ Truncate/Pad to max_length
â†’ Return input_ids + attention_mask
```

**Output Batch:**
```python
{
    'image': Tensor[B, 3, 224, 224],
    'input_ids': Tensor[B, max_length],
    'attention_mask': Tensor[B, max_length],
    'label': Tensor[B]
}
```

---

### Design Decisions

#### Why Late Fusion?

**Pros:**
- Each modality learns rich representations independently
- Easier to debug and interpret
- Better for imbalanced modalities
- Simpler architecture

**Cons:**
- No cross-modal interactions during feature extraction
- May miss early fusion patterns

**Alternative Considered:** Cross-modal attention (more complex, potentially better performance)

#### Why EfficientNet-B0?

**Pros:**
- Efficient (good accuracy/parameter ratio)
- Fast inference
- Pretrained on ImageNet
- Standard input size (224x224)

**Alternatives:** ResNet50 (larger), ViT (requires more data), MobileNet (less accurate)

#### Why DistilBERT?

**Pros:**
- 40% smaller than BERT
- 60% faster inference
- 97% of BERT's performance
- Good for production

**Alternatives:** BERT (larger, slower), RoBERTa (better but larger), TinyBERT (faster but less accurate)

---

## Performance

### Model Size

| Component | Parameters |
|-----------|------------|
| EfficientNet-B0 | ~4M |
| DistilBERT | ~66M |
| Projection Heads | ~1M |
| Fusion Layer | ~0.5M |
| **Total** | **~71M** |

### Expected Results

| Metric | Phase 1 (Frozen) | Phase 2 (Fine-Tuned) |
|--------|------------------|----------------------|
| **Validation Accuracy** | ~74.1% | **84.2%** |
| **Training Accuracy** | ~72.0% | **92.6%** |
| **F1-Score** | ~0.70 | **0.83-0.90** |
| **ROC-AUC** | ~0.80 | **0.90-0.95** |

### Memory Requirements

| Batch Size | GPU Memory (Mixed Precision) |
|------------|------------------------------|
| 8 | ~4 GB |
| 16 | ~6 GB |
| 32 | ~10 GB |
| 64 | ~18 GB |

### Training Speed (V100 GPU)

| Configuration | Samples/sec |
|---------------|-------------|
| Frozen backbones | ~80 |
| Fine-tuning | ~40 |

---

## Troubleshooting

### Training Issues

#### CUDA Out of Memory

```bash
# Reduce batch size
python src/training/train.py training.batch_size=16

# Freeze backbones
python src/training/train.py model.freeze_vision=true model.freeze_text=true
```

#### Slow Training

```bash
# Increase workers
python src/training/train.py training.num_workers=8

# Check GPU availability
python -c "import torch; print(torch.cuda.is_available())"
```

#### Poor Performance

- Check data quality and balance
- Increase num_epochs
- Adjust learning_rate and weight_decay
- Try different fusion_hidden_dim values

---

### Inference Issues

#### Checkpoint Not Found

```bash
# Check checkpoint path
ls -lh src/training/checkpoints/

# Use correct path
python inference.py inference.checkpoint.path=checkpoints/your_model.pth
```

#### Column Not Found in CSV

```bash
# Check CSV columns
head -n 1 data/test.csv

# Update column names
python inference.py \
  data.text_column=your_text_column \
  data.image_column=your_image_column
```

#### CUDA Out of Memory

```bash
# Use CPU instead
python inference.py inference.checkpoint.device=cpu
```

---

### Deployment Issues

#### Docker Container Not Starting

```bash
# Check logs
docker logs fake-news-api

# Verify port availability
lsof -i :8000

# Use different port
docker run -d -p 8080:8000 fake-news-api:v1_onnx
```

#### Kubernetes Pod Not Running

```bash
# Check pod status
kubectl describe pod fake-news-deployment-xxx

# Check logs
kubectl logs fake-news-deployment-xxx

# Delete and recreate
kubectl delete -f k8s-onnx-deployment.yaml
kubectl apply -f k8s-onnx-deployment.yaml
```

---




## Advanced Usage

### Transfer Learning

```bash
# Freeze backbones for faster training
python src/training/train.py \
  model.freeze_vision=true \
  model.freeze_text=true
```

### Custom Model Architecture

Modify `src/models/multimodal_net.py` to experiment with:
- Different vision encoders (EfficientNet, ViT, ResNet)
- Different text encoders (BERT, RoBERTa, ALBERT)
- Alternative fusion strategies (attention, bilinear)

### Hyperparameter Tuning

Use MLflow with Optuna or Ray Tune for automated hyperparameter search.

### Model Export

**ONNX:**
```bash
python export_onnx.py
```

---

## File Size Guidelines

| Size | Recommendation |
|------|----------------|
| < 10MB | Regular Git |
| 10MB - 100MB | Git (with caution) or Git LFS |
| 100MB - 2GB | Git LFS |
| > 2GB | DVC |
| Datasets | Always DVC |

---

## Storage Options

| Option | Free Tier | Best For |
|--------|-----------|----------|
| Google Drive | 15GB | Personal projects |
| AWS S3 | 5GB (12mo) | Production |
| GitHub LFS | 1GB | Small checkpoints |
| Local | Unlimited | Testing |

---

## Best Practices

### Development

- Use MLflow to track all experiments
- Save configurations with Hydra
- Test on small sample first before full dataset
- Use batch mode inference for large datasets
- Set appropriate threshold based on use case

### Deployment

- Use ONNX for production (smaller, faster)
- Deploy to Kubernetes for scalability
- Monitor API logs and performance
- Version all models with Git tags
- Use health checks in production

### Versioning

- Use DVC for datasets (data/images/)
- Use Git LFS for model checkpoints (*.pth)
- Tag important versions (git tag)
- Keep .dvc files in Git
- Document data changes in commit messages
- Never commit large files directly to Git

---

## References

### Papers

- **EfficientNet:** Tan & Le, 2019 - [Efficientnet: Rethinking model scaling for convolutional neural networks](https://arxiv.org/abs/1905.11946)
- **DistilBERT:** Sanh et al., 2019 - [DistilBERT, a distilled version of BERT](https://arxiv.org/abs/1910.01108)
- **Fakeddit Dataset:** Nakamura et al., 2020 - [Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection](https://arxiv.org/abs/1911.03854)

### Tools & Frameworks

- **PyTorch:** https://pytorch.org/
- **Transformers (Hugging Face):** https://huggingface.co/transformers/
- **MLflow:** https://mlflow.org/
- **Hydra:** https://hydra.cc/
- **FastAPI:** https://fastapi.tiangolo.com/
- **DVC:** https://dvc.org/- **ONNX Runtime:** https://onnxruntime.ai/

---

## Contact & Support

For questions or issues:
- Open a GitHub issue
- Check existing documentation in the repository
- Review MLflow experiment logs
**Author:** Rafael Bucio
**Project:** MLOps Zoomcamp - Capstone Project  
**Date:** December 2025


---

## Acknowledgments

This project was created for the MLOps Zoomcamp course by **DataTalks.Club**

---

**Last Updated:** 2025-12-10
**Version:** 1.7
**License:** MIT

---

## Quick Start Commands

```bash
# Setup
git clone <repo-url> && cd final-project
git  dvc pull
pip install -r requirements-dev.txt

# Training
python src/training/train.py
mlflow ui --backend-store-uri sqlite:///src/training/mlflow.db

# Inference
python inference.py inference.mode=single \
  inference.single.image_path=test.jpg \
  inference.single.text="Test news"

# API
uvicorn api:app --reload

# Docker
docker build -t fake-news-api:v1_onnx -f Dockerfile-onnx .
docker run -d -p 8000:8000 fake-news-api:v1_onnx

# Kubernetes
kubectl apply -f k8s-onnx-deployment.yaml
kubectl port-forward service/fake-news-service 8000:80
```

---

**End of Documentation**
